#!/usr/bin/env python3
"""
Ejemplo de scraping de productos usando magnetic proxy.

Este script demuestra c√≥mo usar los m√≥dulos del proyecto para hacer scraping
de productos de manera √©tica y legal a trav√©s del proxy magnetic.

Uso:
    python examples/product_scraper.py
"""

import sys
import os
from pathlib import Path

# Agregar el directorio ra√≠z al path para importar m√≥dulos
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.config.settings import ProxyConfig, ScraperConfig
from src.proxy.client import ProxyClient
from src.scraper.parser import parse_products_page, get_next_page_url


def main():
    """
    Funci√≥n principal que orquesta el scraping de productos.
    """
    print("=" * 60)
    print("Ejemplo de Scraping de Productos con Magnetic Proxy")
    print("=" * 60)
    print()
    
    # Configuraci√≥n del proxy
    # Opci√≥n 1: Desde variables de entorno (recomendado para producci√≥n)
    try:
        proxy_config = ProxyConfig.from_env()
        print("‚úì Configuraci√≥n del proxy cargada desde variables de entorno")
    except ValueError:
        # Opci√≥n 2: Configuraci√≥n manual (solo para ejemplos/testing)
        print("‚ö† Variables de entorno no encontradas, usando configuraci√≥n manual")
        print("  Para usar variables de entorno, configura:")
        print("    export MAGNETIC_PROXY_USER='tu-usuario'")
        print("    export MAGNETIC_PROXY_PASSWORD='tu-password'")
        print()
        
        # Opci√≥n 2: Configuraci√≥n manual (solo para ejemplos/testing)
        # Reemplaza con tus credenciales reales o crea un archivo .env
        proxy_config = ProxyConfig(
            user="customer-carli.f.roman-cc-us-hardcountry-true-sessid-proxy-sample-sesstime-1",
            password="<proxy_user_password>"  # ‚ö†Ô∏è Reemplaza con tu password real
        )
    
    # Configuraci√≥n del scraper
    scraper_config = ScraperConfig(
        base_url="https://books.toscrape.com",
        timeout=10,
        delay_between_requests=1.0
    )
    
    # Crear cliente del proxy
    client = ProxyClient(proxy_config, delay=scraper_config.delay_between_requests)
    
    print(f"‚úì Cliente del proxy inicializado")
    print(f"‚úì URL base: {scraper_config.base_url}")
    print()
    
    # Iniciar scraping
    all_products = []
    current_url = scraper_config.base_url
    max_pages = 2  # Limitar a 2 p√°ginas para el ejemplo
    
    try:
        page_count = 0
        while current_url and page_count < max_pages:
            page_count += 1
            print(f"üìÑ Obteniendo p√°gina {page_count}: {current_url}")
            
            # Hacer request a trav√©s del proxy
            response = client.get(current_url)
            print(f"‚úì Respuesta recibida (status: {response.status_code})")
            
            # Parsear productos de la p√°gina
            products = parse_products_page(response.text, scraper_config.base_url)
            print(f"‚úì {len(products)} productos encontrados en esta p√°gina")
            
            all_products.extend(products)
            
            # Mostrar algunos productos de ejemplo
            if products:
                print("\n  Ejemplos de productos encontrados:")
                for i, product in enumerate(products[:3], 1):
                    print(f"    {i}. {product['title']}")
                    print(f"       Precio: {product['price']}")
                    print(f"       Disponibilidad: {product['availability']}")
                    print(f"       Rating: {product['rating']}")
                    print()
            
            # Buscar siguiente p√°gina
            next_url = get_next_page_url(response.text, scraper_config.base_url)
            if next_url:
                current_url = next_url
                print(f"‚Üí Siguiente p√°gina encontrada: {next_url}")
            else:
                print("‚Üí No hay m√°s p√°ginas")
                break
            
            print("-" * 60)
            print()
        
        # Resumen final
        print("=" * 60)
        print("RESUMEN")
        print("=" * 60)
        print(f"Total de productos encontrados: {len(all_products)}")
        print(f"P√°ginas procesadas: {page_count}")
        print()
        
        # Mostrar algunos productos destacados
        if all_products:
            print("Algunos productos encontrados:")
            for i, product in enumerate(all_products[:5], 1):
                print(f"\n{i}. {product['title']}")
                print(f"   Precio: {product['price']}")
                print(f"   Disponibilidad: {product['availability']}")
                print(f"   Rating: {product['rating']}")
                if product['link']:
                    print(f"   Link: {product['link']}")
        
    except Exception as e:
        print(f"\n‚ùå Error durante el scraping: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

